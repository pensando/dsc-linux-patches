commit 51696d346c49c6cf4f29e9b20d6e15832a2e3408
Author: Will Deacon <will@kernel.org>
Date:   Thu Aug 22 15:03:45 2019 +0100

    arm64: tlb: Ensure we execute an ISB following walk cache invalidation
    
    05f2d2f83b5a ("arm64: tlbflush: Introduce __flush_tlb_kernel_pgtable")
    added a new TLB invalidation helper which is used when freeing
    intermediate levels of page table used for kernel mappings, but is
    missing the required ISB instruction after completion of the TLBI
    instruction.
    
    Add the missing barrier.
    
    Cc: <stable@vger.kernel.org>
    Fixes: 05f2d2f83b5a ("arm64: tlbflush: Introduce __flush_tlb_kernel_pgtable")
    Reviewed-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/arch/arm64/include/asm/tlbflush.h b/arch/arm64/include/asm/tlbflush.h
index 8af7a85..bc39490 100644
--- a/arch/arm64/include/asm/tlbflush.h
+++ b/arch/arm64/include/asm/tlbflush.h
@@ -251,6 +251,7 @@ static inline void __flush_tlb_kernel_pgtable(unsigned long kaddr)
 	dsb(ishst);
 	__tlbi(vaae1is, addr);
 	dsb(ish);
+	isb();
 }
 #endif
 
